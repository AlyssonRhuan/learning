# Defining the problem

There is a range of problems to solve when designing a networking solution for distributed orchestration system, including:

- Inter-pod communication between containers.
- Communication between pods both on one node and on different nodes.
- Accessibility of services (one of the Kubernetes resources we discussed earlier) from pods.
- And, last but not least, ingress connections from the outside world to a cluster.

# Docker communication

By default, Docker uses a private host-level virtual bridhe with an addres range allocated by NAT from on of the standard private blocks. 
Theses days, Docker supports more networking models, but the above bridge model has been the default mode histprically and many ohter orchestration tools rely on it.

![image](https://user-images.githubusercontent.com/13942355/136535592-2320f25d-30d9-4e44-9057-fcf26bf30dad.png)

Within the model, containers have direct network connectivity between each other within one host, but they can't directly reach out to containers on other hosts, and they can't be accessed from and outside network.

To address this problem, Docker uses dynamic port allocation where a ramdom port from the host IP address is mapped to a container ip:port pair. This allows you to bridge the gap between a container's network and the host's network, but it requires the use of LoadBalancer with ip:port registration of backend containers.

![image](https://user-images.githubusercontent.com/13942355/136536258-366c6a53-01fb-4412-8aec-a2392a5707de.png)

# EKS Communication

In Kubernetes networking, the IP address with which a contianer identifies is the same IP address that others see, and all containers can communicate with all other container and with nodes without NAT. Because pods are the basic building block of Kubernetes, Kubernetesapplies an IP address from the network to a pod rather than to a single container. Containers within a pod share a Linux namespace and can communicate with each other using localhost. For pod-to-pod communication within the host, the pods'namespace are connected using a Linux Virtual Ethernet Device (veth). Each node gets a network range for containers, and each pod gets an IP address in that range. Now containers on the same host can communicate.

What about pods on other nodes? To simplify inter-node communication, EKS integrates Amazon Virtual Private Cloud (VPC) networking into Kubernetes through the Amazon VPC Container Network Interface (CNI) plugin for Kubernetes. This CNI plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. This CNI plugin is an open-source project that is maintained on GitHub. The CNI leverages EC2’s ability to provision multiple elastic network interfaces (ENIs) to a host instance, each with multiple secondary IP addresses, to get multiple IP addresses allocated from the VPC pool. It then hands out these IP addresses to pods on the host, connects the ENI to the veth port created on the pod, and lets the Linux kernel take care of the rest. Thus every pod has a real, routable IP address from VPC and can easily reach out to other pods, nodes, or AWS services.

EKS uses Calico to enable network policies. A “calico-node” agent is deployed on each node. This allows routing information to propagate amongst nodes using the standard BGP routing protocol. By aligning the plug-in veth naming convention with Calico, the calico-node on each host knows which veth belongs to which container and can create policy rules on each container’s interface in the Linux kernel (using iptables/ipsets) as usual. And because the plugin has plumbed the veths in just the right way, all traffic out of the pod will have those rules applied. 

![image](https://user-images.githubusercontent.com/13942355/137465678-6bd45506-f988-4079-a62a-880b35d24171.png)

# Incorporating Services

S service provides a constant IP address and port as an entry point to a gourp of pods. Each service has an IP address and port that will never change for as long as the service exists. Internal or external clients can reach out to your applicationrunning in a group of pods by connecting to the service IP and port, and those connections are then routed to one of the pods backing that service.

![image](https://user-images.githubusercontent.com/13942355/137466952-aa532b59-a01f-4fb7-bd3b-edba69255611.png)

# Inside a service

The most important field in the spec is 'selector', which determines which pods will serve as endpoints for this service. In this case, any pods with the label app=MyApp will be part of this service. In this example, we're also exposing two ports (80 and 443) tha map to container ports 9376 and 9377, respectively.

![image](https://user-images.githubusercontent.com/13942355/137468162-785081d4-0038-4b50-9495-b2cc63a534d2.png)



