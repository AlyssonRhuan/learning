# Rationale for container security

![image](https://user-images.githubusercontent.com/13942355/129880423-96981af4-c3c8-462d-ad04-da7ddff81a5a.png)


- VM’s are more of hardware isolation (no sharing of OS and/or kernel)  while containers are pure OS level virtualization
- Containers are NOT truly sandboxed due to sharing of kernel with the host . Only VMs are . Sadly, speed of boot up comprises security 
- Virtualized hardware isolation creates a much stronger security boundary than namespace (and cgroup) isolation in case of containers. 
- Isolation brought about by containers is easily susceptible for compromise through multiple means 
    - Kernel exploit vulnerabilities 
    - Containers with excessive privileges( like SYS_ADMIN)
    - Containers that access critical mount points 
- True ‘sandboxed’ containers is need of the hour 

# Sandboxing options in the market

- Unikernel 
- Nabla containers
- gVisor
- Kata containers
- AWS Firecracker 

Italicized options are the focus of this preliminary study/PoC

# Where they fit

![image](https://user-images.githubusercontent.com/13942355/129880658-fcb71d6f-5f2e-4f0f-863d-56082710ba0c.png)

# Architectural comparison

![image](https://user-images.githubusercontent.com/13942355/129880714-6872f360-0309-4700-bd5e-725f094dfdd6.png)

# Kata container

- VM-based sandbox technology designed for cloud-native applications
- Lightweight VMs that are highly optimized for running containers
- Leverages  kata-runtime on the host to start and configure new containers (and micro VMs for each new container)
- Fully integrated with OCI (low level runtime) , Container Runtime Interface (high level runtime), and Container Networking Interface (CNI)
- AWS firecracker has same vision as Kata , but low-level design and approach is different 
- Host must support nested virtualization for kata-runtime to work and spin up containers with corresponding micro VM

# gvisor

- Sandbox technology leveraging  user space kernel 
- Fully integrated with OCI (low level runtime) , Container Runtime Interface (high level runtime), and Container Networking Interface (CNI)
- Sandboxes by intercepting all the system calls from applications to host kernel and handling them in the user space kernel 
- ‘Sentry’ is the fundamental building block behind user space kernel . Uses Gofer to communicate with host kernel if needed
- Not all native system calls are supported by sentry – applications using unsupported system calls may face issues likely 
- Easier adoption compared to Kata containers. Low level runtime binary (runsc) is the only pre-requisite

# gvisor Poc instructions (Pure Docker)

- `Pre-requisites:` Any Linux machine with docker installed

1. Step #1
```shell
sudo apt-get update && \
sudo apt-get install -y \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common
```
2. Step #2
```shell
curl -fsSL https://gvisor.dev/archive.key | sudo apt-key add –
```
3. Step #3
```shell
sudo add-apt-repository "deb https://storage.googleapis.com/gvisor/releases release main"
```
4. Step #4
```shell
sudo apt-get update && sudo apt-get install -y runsc
```
5. Step #5
Verify /etc/docker/daemon.json for runsc runtime . If not add it and give the path to the binary from previous step 
Launch containers with gvisor :  docker run -ti --runtime runsc alpine uname –a
To test : execute the command dmesg inside the container and outside (on the host) .  Check the output ( kernel message buffer) to see the distinction 

# gvisor Poc instructions (k8S)

- `CAUTION:` GVISOR IS PRETTY UNSTABLE ON KUBERNETES DISTROS APART FROM GKE( PROJECT IS FIXING ISSUES IN GKE FIRST) 
- INSTRUCTIONS GIVEN BELOW THUS HAVE TO BE TAKEN WITH A PINCH OF SALT

- `Pre-requisites:` Any machine with capability to install minikube

1. Step #1
```shell
minikube start --container-runtime=containerd  \
    --docker-opt containerd=/var/run/containerd/containerd.sock
```
2. Step #2
Minikube addons enable gvisor
3. Step #3
Check runtime class is created using kubectl get pod,runtimeclass gvisor –n kube-system
4. Step #4
Launch any container of your choice with parameter ‘runtimeClassName: gvisor’ in the pod spec 
5. Step #5
To test : execute the command dmesg inside the container and outside (on the host) .  Check the output ( kernel message buffer) to see the distinction 

# Comparison

![image](https://user-images.githubusercontent.com/13942355/129881397-bf1cb745-372e-45b3-bc01-2fcdd740f685.png)

