# High-level view of K8S components

![image](https://user-images.githubusercontent.com/13942355/129702889-e10f9074-952e-4dbc-9813-1d1231ffe81e.png)

- API Server is the component processing all the K8s client  commands – such as the ones from kubectl , helm etc ( helm under the hood is just another K8s client)
- Controller Manager is the watchguard component - Different kinds of controller processes run . These include replication controller , daemonset controller , node controller etc 
- Scheduler is the decision-making component when new workload definitions are submitted to the API server
- ETCD is the API server’s DB and single source of truth 
- Above set of components run in the master node
- Worker nodes are solely concerned with running the workloads 
- Kubelet is the ‘brain’ of the worker nodes handling communication with API server and container runtime , interacting with the networking layer 
- Networking in K8s is realization of CNI [ container network interface ] 

# Whats is CNI?

- CNI is a standardized specification defined by K8s workgroup to handle networking ( think of it as being similar to  JSR in Java world)
- Multiple vendors implement CNI – eg: Calico , Flannel , Weave , AWS VPC-CNI , Azure CNI etc 
- All CNI implementations essentially fulfill 3 main requirements of the spec
    - All Pods can communicate with all other Pods without using network address translation (NAT).
    - All Nodes can communicate with all Pods without NAT.
    - The IP that a Pod sees itself as is the same IP that others see it as.

# What dows CNI offer

- CNI in a nutshell provides the solutions for the below networking scenarios
    - Container-to-Container networking
    - Pod-to-Pod networking
    - Pod-to-Service networking
    - Internet-to-Service networking
- CNI essentially makes communication between application workloads possible 
- Without CNI , containerized deployments are … well , useless.

# Networking basics

## Assumed view of networking in a VM

![image](https://user-images.githubusercontent.com/13942355/129708496-d2494b80-ffc7-4412-bb94-2dc58d546093.png)

- We envision a simplistic view of networking where the VM has ethernet interface(s) attached to it.

## Actual view of networking in a VM

![image](https://user-images.githubusercontent.com/13942355/129708543-42e64b35-0683-453e-88ca-87fadc6b7286.png)

- Linux groups networks by namespaces – as a means of isolation
- Think of this as logical networking stacks with its own routes, firewall rules, and network devices.
- A root network namespace is the one bound to every process ( by default) 
- Additional network namespaces can be created using the ip command. Eg : ip netns add ns1
- Network namespaces are mounted in /var/run/netns – Try the commands. The mounts will be created even if there in process attached to use 

 # Networking in Docker (Single Node view)
 
 ![image](https://user-images.githubusercontent.com/13942355/129709394-7c1dfb5a-ce59-433a-8b70-c0cdedab059a.png)

`Note:` ‘Pod1 , Pod2’ are representative of docker networks. Do not confuse them with pods in K8s

- Docker containers have an exclusive network namespace and do not share the root network namespace 
- Typically, each container would have its own network namespace 
- Possible to have multiple containers associated to single network namespace (through –net option)
- Services in a docker compose file are classic examples of this

# Networking in K8S (Single Node view)

![image](https://user-images.githubusercontent.com/13942355/129709580-fd56bf79-376f-4746-8ed0-e1c7092c018c.png)

- Every pod exists in its own network namespace 
- Containers within a pod share the same IP address 
- Communication to another pod in same node (and diff n/w namespace) is through the virtual ethernet device (veth) 
- One end of the veth is in the root n/w NS and the other in the pod n/w NS 
- Communication is made possible by connecting the veth ends in root n/w NS to a common bridge n/w 
- Bridge n/w is an L2 device capable of connecting several network segments 
- Smart rule engine that can decide to which n/w segment a packet must be sent 
- Decisions are cached for efficiency and performance reasons 

# Pod-Pod Communication (Same node)

![Presentation2](https://user-images.githubusercontent.com/13942355/129874497-5ad7b4ec-5a58-489e-bff9-4c5869974308.gif)

- Pod 1 sends a packet to its own Ethernet device eth0 which is available as the default device for the Pod. For Pod 1, eth0 is connected via a virtual Ethernet device to the root namespace, veth0 (1).
- The packet is then handled by the bridge network (2) 
- The bridge resolves the correct network segment to send the packet to — veth1 (3)
- When the packet reaches the virtual device veth1, it is forwarded directly to Pod 2’s namespace and the eth0 device within that namespace (4)

Note : Throughout this traffic flow, each Pod is communicating only with eth0 on localhost and the traffic is routed to the correct Pod.

# Pod-Pod Communication (Acriss node)

![Presentation3](https://user-images.githubusercontent.com/13942355/129875452-a1947c28-6aea-474d-8194-5d5f7ea567e4.gif)

- Pod 1 sends a packet to its own Ethernet device eth0 which is available as the default device for the Pod. For Pod 1, eth0 is connected via a virtual Ethernet device to the root namespace, veth0 (1).
- The packet is then handled by the bridge network (2) 
- The bridge is unable to resolve the network segment to which packet must be routed , hence sends the packet via default route – eth0 of root (3)
- CNI implementation takes care of routing the packet to the correct Node based on the CIDR block assigned to the Node (4)
- When the packet reaches the virtual device veth1, it is forwarded directly to Pod 2’s namespace and the eth0 device within that namespace (4)
- The packet enters the root namespace of the destination Node (eth0 on VM 2), where it is routed through the bridge to the correct virtual Ethernet device (5). 
- The route completes by flowing through the virtual Ethernet device’s pair residing within Pod 4’s namespace (6).

# Pod-K8s Service communication

![Presentation4](https://user-images.githubusercontent.com/13942355/129875959-8fe74f99-3069-4e43-9744-0f27f7d889d5.gif)

- Pod 1 sends a packet to its own Ethernet device eth0 which is available as the default device for the Pod. For Pod 1, eth0 is connected via a virtual Ethernet device to the root namespace, veth0 (1).
- The packet is then handled by the bridge network (2) 
- The bridge is unable to resolve the network segment to which packet must be routed , hence sends the packet via default route – eth0 of root (3)
- Before being routed, iptables inspection kicks in and this uses the rules installed on the Node by kube-proxy in response to K8s events (pod/service/endpoint creation) to rewrite the destination of the packet from the Service IP to a specific Pod IP (4)
- Conversion of service IP to pod IP is accomplished at this stage 
- Traffic then flows to the Pod using the Pod-to-Pod routing (5)

# Pod-Ext Service communication

![Presentation5](https://user-images.githubusercontent.com/13942355/129876316-30a6845f-61ff-4e69-b021-f90eeed5add4.gif)

- Pod 1 sends a packet to its own Ethernet device eth0 which is available as the default device for the Pod. For Pod 1, eth0 is connected via a virtual Ethernet device to the root namespace, veth0 (1).
- The packet is then handled by the bridge network (2) 
- The bridge is unable to resolve the network segment to which packet must be routed , hence sends the packet via default route – eth0 of root (3)
- Before reaching the root namespace’s Ethernet device  iptables mangles the packet (3). 
- In this case, the source IP address of the packet is a Pod IP and the gateway only understands IP addresses that are connected to VMs. 
- The solution is to have iptables perform a source NAT — changing the packet source 
- With the correct source IP in place, the packet leaves the VM (4) and reaches the Internet gateway (5). 
- The Internet gateway will do does another NAT rewriting - the source IP from a VM internal IP to an external IP. 
- Finally, the packet reaches  the public Internet (6).
 
On the way back, the packet follows the same path, and any source IP mangling is undone so that each layer of the system receives the IP address that it understands: VM-internal at the Node or VM level, and a Pod IP within a Pod’s namespace.

# Internet-K8s service communication (no ingress)

![Presentation6](https://user-images.githubusercontent.com/13942355/129876786-a430495f-ed4d-4ac2-a3b1-929b649128b5.gif)

- Incoming traffic (1) is directed at the load balancer for designated Service. 
- Once the load balancer receives the packet (2) it picks a VM at random. 
- In this case, assume it picks VM 2 (3)
- The iptables rules running on the VM will direct the packet to the correct Pod 
- Iptables also does the correct NAT and forwards the packet on to the correct Pod (4)

# Ingress controllers

![image](https://user-images.githubusercontent.com/13942355/129876857-51f751da-034e-4371-a131-7d1a01688d00.png)

- Upon creation, (1) an Ingress Controller watches for Ingress events from the Kubernetes API server.
- When ingress objects are found, the controller proceeds to create needed rule/route – service mappings (2)
- Alongside the rule/route-service mappings , the pod IP – service mappings are also created (3)
- When new ingress objects are created , the same operations are done (4) and (5)
